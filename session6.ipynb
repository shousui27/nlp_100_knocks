{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f04964c",
   "metadata": {},
   "source": [
    "## 50. データの入手・整形Permalink\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "抽出された事例をランダムに並び替える．\n",
    "抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b56890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def sample_data(data):\n",
    "    valid_data = data.sample(int(len(data)*0.1))\n",
    "    test_data = data[~data[\"ID\"].isin(valid_data[\"ID\"])].sample(int(len(data)*0.1))\n",
    "    train_data = data[~data[\"ID\"].isin(valid_data[\"ID\"]) & ~data[\"ID\"].isin(test_data[\"ID\"])]\n",
    "    return train_data,valid_data,test_data\n",
    "data = pd.read_csv(\"./data/NewsAggregatorDataset/newsCorpora.csv\",sep=\"\\t\",header=None)\n",
    "data.columns = [\"ID\",\"TITLE\",\"URL\",\"PUBLISHER\",\"CATEGORY\",\"STORY\",\"HOSTNAME\",\"TIMESTAMP\"]\n",
    "data[\"TITLE\"] = data[\"TITLE\"].str.lower()\n",
    "data = data[data[\"PUBLISHER\"].isin([\"Reuters\", \"Huffington Post\", \"Businessweek\", \"Contactmusic.com\", \"Daily Mail\"])]\n",
    "data = data.reset_index(drop=True)\n",
    "train_data,valid_data,test_data = sample_data(data)\n",
    "print(train_data.shape)\n",
    "print(valid_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894c6a8",
   "metadata": {},
   "source": [
    "## 51. 特徴量抽出Permalink\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(series:pd.Series,words):\n",
    "    vectors = []\n",
    "    for title in series:\n",
    "        vec = [0 for _index in range(len(words))]\n",
    "        for word in title.split(\" \"):\n",
    "            df = words[words == word]\n",
    "            if len(df) != 0:\n",
    "                vec[df.index[0]] = 1\n",
    "        vectors.append(vec)\n",
    "    vector_df = pd.DataFrame(vectors)\n",
    "    vector_df.columns = [f\"words_{column}\" for column in vector_df.columns]\n",
    "    return vector_df\n",
    "\n",
    "def mk_feature_df(data,words):\n",
    "    train_vector = text2vec(data[\"TITLE\"],words)\n",
    "\n",
    "    category_vector = pd.get_dummies(data[\"PUBLISHER\"])\n",
    "    features = pd.concat([data.reset_index(drop=True)[[\"ID\",\"CATEGORY\"]],category_vector.reset_index(drop=True),train_vector.reset_index(drop=True)],axis=1)\n",
    "    features[\"CATEGORY\"] = features[\"CATEGORY\"].replace({'b':0, 't':1, 'e':2, 'm':3})\n",
    "    return features\n",
    "\n",
    "words = pd.Series([word for text in train_data[\"TITLE\"] for word in text.split(\" \")])\n",
    "words = words.drop_duplicates().reset_index()[0]\n",
    "\n",
    "train_features = mk_feature_df(train_data,words)\n",
    "valid_features = mk_feature_df(valid_data,words)\n",
    "test_features = mk_feature_df(test_data,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561d34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_XY_data(train_data):\n",
    "    X_train = train_data.drop([\"ID\",\"CATEGORY\"],axis=1)\n",
    "    Y_train = train_data[\"CATEGORY\"]\n",
    "    return X_train,Y_train\n",
    "X_train,Y_train = mk_XY_data(train_features)\n",
    "X_valid,Y_valid = mk_XY_data(valid_features)\n",
    "X_test,Y_test = mk_XY_data(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef3785",
   "metadata": {},
   "source": [
    "## 52. 学習Permalink\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(max_iter=1500)\n",
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc259ecf",
   "metadata": {},
   "source": [
    "## 53. 予測Permalink\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4378c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = mk_feature_df(valid_data,words)\n",
    "X_valid,Y_valid = mk_XY_data(valid_features)\n",
    "lr.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc4a583",
   "metadata": {},
   "source": [
    "## 54. 正解率の計測Permalink\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(lr,test_data,grand_truth):\n",
    "    truth = pd.DataFrame(grand_truth)\n",
    "    truth[\"pred\"] = lr.predict(test_data)\n",
    "    return len(truth[truth[\"CATEGORY\"] == truth[\"pred\"]]) / len(truth)\n",
    "print(get_accuracy(lr,X_train,Y_train))\n",
    "print(get_accuracy(lr,X_valid,Y_valid))\n",
    "print(get_accuracy(lr,X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3158d32",
   "metadata": {},
   "source": [
    "## 55. 混同行列の作成Permalink\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2dd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def get_confusion_matrix(test_data,grand_truth):\n",
    "    cm = confusion_matrix(grand_truth, lr.predict(test_data))\n",
    "    cm = pd.DataFrame(cm)\n",
    "    cm.columns = ['b','t','e','m']\n",
    "    cm.index = ['b','t','e','m']\n",
    "    return cm\n",
    "get_confusion_matrix(X_valid,Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78af5c5",
   "metadata": {},
   "source": [
    "## 56. 適合率，再現率，F1スコアの計測Permalink\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37567cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "\n",
    "Y_pred = lr.predict(X_valid)\n",
    "print(f\"precision macro : {precision_score(Y_valid.values, Y_pred, average='macro')}\")\n",
    "print(f\"precision micro : {precision_score(Y_valid.values, Y_pred, average='micro')}\")\n",
    "\n",
    "print(f\"recall macro : {recall_score(Y_valid.values, Y_pred, average='macro')}\")\n",
    "print(f\"recall micro : {recall_score(Y_valid.values, Y_pred, average='micro')}\")\n",
    "\n",
    "print(f\"f1 macro : {f1_score(Y_valid.values, Y_pred, average='macro')}\")\n",
    "print(f\"f1 micro : {f1_score(Y_valid.values, Y_pred, average='micro')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bc358",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be48dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df = pd.DataFrame(lr.coef_).T\n",
    "weight_df.columns = ['b','t','e','m']\n",
    "for column in weight_df.columns:\n",
    "    print(column)\n",
    "    print(words.loc[weight_df[column].sort_values(ascending=False).head(10).index])\n",
    "    print()\n",
    "    print(words.loc[weight_df[column].sort_values(ascending=True).head(10).index])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3261e3d2",
   "metadata": {},
   "source": [
    "## 58. 正則化パラメータの変更Permalink\n",
    "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937a738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = [x*0.01 for x in range(1,10)]\n",
    "y = []\n",
    "for x in params:\n",
    "    print(x)\n",
    "    lr = LogisticRegression(max_iter=1500,C=x)\n",
    "    lr.fit(X_train, Y_train)\n",
    "    y.append(get_accuracy(lr,X_valid,Y_valid))\n",
    "plt.scatter(x=params,y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973083cb",
   "metadata": {},
   "source": [
    "## 59. ハイパーパラメータの探索Permalink\n",
    "学習アルゴリズムや学習パラメータを変えながら，カテゴリ分類モデルを学習せよ．検証データ上の正解率が最も高くなる学習アルゴリズム・パラメータを求めよ．また，その学習アルゴリズム・パラメータを用いたときの評価データ上の正解率を求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d32bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {\"C\": [10 ** i for i in range(-2, 2)]\n",
    "         }\n",
    "model = GridSearchCV(LogisticRegression(max_iter=1500),params)\n",
    "model.fit(X_train, Y_train)\n",
    "print(get_accuracy(model,X_train,Y_train))\n",
    "print(get_accuracy(model,X_valid,Y_valid))\n",
    "print(get_accuracy(model,X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4c964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
